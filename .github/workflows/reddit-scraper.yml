name: Elite Reddit Scraper with Multi-Key Support

on:
  workflow_dispatch:
    inputs:
      target_count:
        description: 'Number of influencers to scrape (e.g., 2500, 5000)'
        required: true
        default: '2500'
        type: string
      max_runtime_hours:
        description: 'Maximum runtime in hours (1-8 hours)'
        required: true
        default: '4'
        type: choice
        options:
        - '1'
        - '2'
        - '3'
        - '4'
        - '5'
        - '6'
        - '7'
        - '8'
      save_interval:
        description: 'Save progress every N accounts (recommended: 25-100)'
        required: false
        default: '50'
        type: string
  schedule:
    # Run automatically every Sunday at 2 AM UTC (optional)
    - cron: '0 2 * * 0'

jobs:
  scrape-reddit:
    runs-on: ubuntu-latest
    timeout-minutes: ${{ fromJson(github.event.inputs.max_runtime_hours || '4') * 60 + 30 }}  # Input hours + 30min buffer
    
    steps:
    - name: ğŸ“¥ Checkout repository
      uses: actions/checkout@v4
      
    - name: ğŸ Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: ğŸ“¦ Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests
        echo "âœ… Dependencies installed successfully"
    
    - name: ğŸ”§ Configure environment
      run: |
        echo "TARGET_COUNT=${{ github.event.inputs.target_count || '2500' }}" >> $GITHUB_ENV
        echo "MAX_RUNTIME_HOURS=${{ github.event.inputs.max_runtime_hours || '4' }}" >> $GITHUB_ENV
        echo "SAVE_INTERVAL=${{ github.event.inputs.save_interval || '50' }}" >> $GITHUB_ENV
        
        # Multi-key ScraperAPI support
        echo "SCRAPE_KEY=${{ secrets.SCRAPE_KEY }}" >> $GITHUB_ENV
        echo "SCRAPERAPI_KEY=${{ secrets.SCRAPERAPI_KEY }}" >> $GITHUB_ENV
        echo "SCRAPERAPI_KEY_2=${{ secrets.SCRAPERAPI_KEY_2 }}" >> $GITHUB_ENV
        echo "SCRAPERAPI_KEY_3=${{ secrets.SCRAPERAPI_KEY_3 }}" >> $GITHUB_ENV
        echo "SCRAPERAPI_KEY_4=${{ secrets.SCRAPERAPI_KEY_4 }}" >> $GITHUB_ENV
        echo "SCRAPERAPI_KEY_5=${{ secrets.SCRAPERAPI_KEY_5 }}" >> $GITHUB_ENV
        echo "SCRAPERAPI_KEYS=${{ secrets.SCRAPERAPI_KEYS }}" >> $GITHUB_ENV
        
        echo "ğŸ”‘ Environment configured with multi-key support"
        echo "â° Max runtime: ${{ github.event.inputs.max_runtime_hours || '4' }} hours"
        echo "ğŸ¯ Target count: ${{ github.event.inputs.target_count || '2500' }} accounts"
    
    - name: ğŸ” Validate API keys and capacity
      run: |
        echo "ğŸ”‘ Checking available ScraperAPI keys..."
        key_count=0
        total_capacity=0
        
        # Check SCRAPE_KEY (your original key)
        if [ -n "$SCRAPE_KEY" ]; then
          echo "âœ… SCRAPE_KEY (primary): ...$(echo $SCRAPE_KEY | tail -c 9)"
          key_count=$((key_count + 1))
          total_capacity=$((total_capacity + 5000))
        fi
        
        # Check SCRAPERAPI_KEY
        if [ -n "$SCRAPERAPI_KEY" ]; then
          echo "âœ… SCRAPERAPI_KEY: ...$(echo $SCRAPERAPI_KEY | tail -c 9)"
          key_count=$((key_count + 1))
          total_capacity=$((total_capacity + 5000))
        fi
        
        # Check additional keys (2-5)
        for i in 2 3 4 5; do
          key_var="SCRAPERAPI_KEY_$i"
          key_value=$(eval echo \$$key_var)
          if [ -n "$key_value" ]; then
            echo "âœ… SCRAPERAPI_KEY_$i: ...$(echo $key_value | tail -c 9)"
            key_count=$((key_count + 1))
            total_capacity=$((total_capacity + 5000))
          fi
        done
        
        # Check comma-separated keys
        if [ -n "$SCRAPERAPI_KEYS" ]; then
          comma_keys=$(echo "$SCRAPERAPI_KEYS" | tr ',' '\n' | wc -l)
          echo "âœ… SCRAPERAPI_KEYS: $comma_keys additional keys found"
          key_count=$((key_count + comma_keys))
          total_capacity=$((total_capacity + comma_keys * 5000))
        fi
        
        echo "ğŸ“Š CAPACITY ANALYSIS:"
        echo "  ğŸ”‘ Total API keys: $key_count"
        echo "  ğŸ’° Estimated capacity: $total_capacity accounts"
        echo "  ğŸ¯ Target requested: $TARGET_COUNT accounts"
        echo "  â° Runtime limit: $MAX_RUNTIME_HOURS hours"
        
        if [ "$key_count" -eq 0 ]; then
          echo "âŒ ERROR: No API keys found!"
          echo "Please add at least one of these secrets to your repository:"
          echo "  â€¢ SCRAPE_KEY (your original key)"
          echo "  â€¢ SCRAPERAPI_KEY"
          echo "  â€¢ SCRAPERAPI_KEY_2, SCRAPERAPI_KEY_3, etc."
          echo "  â€¢ SCRAPERAPI_KEYS (comma-separated list)"
          exit 1
        fi
        
        if [ "$TARGET_COUNT" -gt "$total_capacity" ]; then
          echo "âš ï¸ WARNING: Target ($TARGET_COUNT) exceeds estimated capacity ($total_capacity)"
          echo "Consider adding more API keys or reducing target count"
        else
          echo "âœ… Capacity check passed - sufficient keys available"
        fi
    
    - name: ğŸ“Š Check for previous progress
      run: |
        echo "ğŸ” Checking for previous session data..."
        
        if [ -f "elite_scraper_progress.json" ]; then
          echo "ğŸ”„ Previous progress file found"
          python3 -c "
import json, os
try:
    with open('elite_scraper_progress.json', 'r') as f:
        data = json.load(f)
    print(f'ğŸ“ˆ Previous session stats:')
    print(f'  â€¢ Accounts scraped: {data.get(\"total_scraped\", 0)}')
    print(f'  â€¢ Users processed: {len(data.get(\"scraped_users\", []))}')
    print(f'  â€¢ Failed users: {len(data.get(\"failed_users\", []))}')
    print(f'  â€¢ Last run: {data.get(\"timestamp\", \"Unknown\")}')
    if 'multi_key_stats' in data:
        stats = data['multi_key_stats']
        print(f'  â€¢ Credits used: {stats.get(\"total_credits_used\", 0)}')
        print(f'  â€¢ Keys used: {stats.get(\"total_keys\", 0)}')
except Exception as e:
    print(f'âš ï¸ Could not read progress file: {e}')
"
        else
          echo "ğŸ†• Starting fresh scraping session"
        fi
        
        if [ -f "reddit_elite_influencers.csv" ]; then
          echo "ğŸ“„ Existing CSV file found"
          existing_count=$(($(wc -l < reddit_elite_influencers.csv) - 1))
          echo "  â€¢ Current CSV contains: $existing_count accounts"
        fi
    
    - name: ğŸš€ Run Elite Reddit Scraper with Timeout
      id: scraper_run
      run: |
        echo "ğŸ† Starting Elite Reddit Scraper with Multi-Key Support"
        echo "=============================================="
        echo "ğŸ¯ Target: $TARGET_COUNT elite accounts (10K+ karma)"
        echo "â° Max runtime: $MAX_RUNTIME_HOURS hours"
        echo "ğŸ’¾ Save interval: $SAVE_INTERVAL accounts"
        echo "ğŸ”‘ API keys configured: Multi-key rotation enabled"
        echo "=============================================="
        
        # Create directories
        mkdir -p logs backups
        
        # Calculate timeout in seconds (input hours * 3600 - 5 min buffer)
        timeout_seconds=$(( ($MAX_RUNTIME_HOURS * 3600) - 300 ))
        echo "â±ï¸ Timeout set to: $timeout_seconds seconds"
        
        # Run scraper with proper timeout and error handling
        set +e  # Don't exit on error
        
        timeout $timeout_seconds python3 oldone.py 2>&1 | tee logs/scraper_output.log
        exit_code=$?
        
        echo "ğŸ” Scraper finished with exit code: $exit_code"
        
        # Handle different exit scenarios
        if [ $exit_code -eq 0 ]; then
          echo "âœ… Scraper completed successfully!"
          echo "SCRAPER_STATUS=success" >> $GITHUB_ENV
        elif [ $exit_code -eq 124 ]; then
          echo "â° Scraper reached maximum runtime ($MAX_RUNTIME_HOURS hours)"
          echo "ğŸ’¾ Progress should be saved automatically"
          echo "SCRAPER_STATUS=timeout" >> $GITHUB_ENV
        elif [ $exit_code -eq 130 ]; then
          echo "ğŸ›‘ Scraper was interrupted (Ctrl+C)"
          echo "SCRAPER_STATUS=interrupted" >> $GITHUB_ENV
        else
          echo "âš ï¸ Scraper exited with error code: $exit_code"
          echo "ğŸ’¾ Attempting to save any collected data..."
          echo "SCRAPER_STATUS=error" >> $GITHUB_ENV
        fi
        
        # Always try to save current state, regardless of exit status
        echo "ğŸ’¾ Final data preservation check..."
        
        set -e  # Re-enable exit on error for remaining steps
      continue-on-error: true  # Don't fail the workflow if scraper has issues
    
    - name: ğŸ“Š Analyze Results and Create Summary
      if: always()  # Always run this step
      run: |
        echo "ğŸ“Š SCRAPING SESSION ANALYSIS"
        echo "============================"
        
        # Initialize counters
        csv_accounts=0
        backup_accounts=0
        interrupted_accounts=0
        error_accounts=0
        
        # Check main CSV
        if [ -f "reddit_elite_influencers.csv" ]; then
          csv_accounts=$(($(wc -l < reddit_elite_influencers.csv) - 1))
          file_size=$(ls -lh reddit_elite_influencers.csv | awk '{print $5}')
          echo "âœ… Main CSV: $csv_accounts elite accounts ($file_size)"
        else
          echo "âŒ Main CSV not found"
        fi
        
        # Check backup files
        for backup_file in backup_reddit_elite_influencers.csv interrupted_reddit_elite_influencers.csv error_reddit_elite_influencers.csv; do
          if [ -f "$backup_file" ]; then
            count=$(($(wc -l < $backup_file) - 1))
            size=$(ls -lh $backup_file | awk '{print $5}')
            echo "ğŸ’¾ $backup_file: $count accounts ($size)"
            
            case $backup_file in
              backup_*) backup_accounts=$count ;;
              interrupted_*) interrupted_accounts=$count ;;
              error_*) error_accounts=$count ;;
            esac
          fi
        done
        
        # Determine best file to use
        max_accounts=$csv_accounts
        best_file="reddit_elite_influencers.csv"
        
        if [ $backup_accounts -gt $max_accounts ]; then
          max_accounts=$backup_accounts
          best_file="backup_reddit_elite_influencers.csv"
        fi
        
        if [ $interrupted_accounts -gt $max_accounts ]; then
          max_accounts=$interrupted_accounts
          best_file="interrupted_reddit_elite_influencers.csv"
        fi
        
        if [ $error_accounts -gt $max_accounts ]; then
          max_accounts=$error_accounts
          best_file="error_reddit_elite_influencers.csv"
        fi
        
        echo "ğŸ† Best result file: $best_file with $max_accounts accounts"
        
        # Create final consolidated CSV if we have any data
        if [ $max_accounts -gt 0 ]; then
          echo "ğŸ“‹ Creating final consolidated results..."
          
          # Copy best file to final results
          cp "$best_file" "FINAL_reddit_elite_influencers.csv"
          
          # Show top performers
          echo "ğŸ¥‡ TOP 15 ELITE ACCOUNTS:"
          echo "Rank | Username              | Tier      | Total Karma | Per Day  | Reach"
          echo "-----|---------------------- |-----------|-------------|----------|----------"
          
          # Skip header and show top 15
          tail -n +2 "FINAL_reddit_elite_influencers.csv" | head -15 | nl -w4 -s'. ' | while IFS=',' read rank username url tier total_karma link_karma comment_karma age_days karma_per_day reach rest; do
            printf "%-4s | %-20s | %-9s | %11s | %8s | %s\n" "$rank" "u/$username" "$tier" "$total_karma" "$karma_per_day" "$reach"
          done
          
          # Calculate tier distribution
          echo ""
          echo "ğŸ† TIER DISTRIBUTION:"
          for tier in LEGENDARY MEGA SUPER MAJOR RISING MICRO; do
            count=$(tail -n +2 "FINAL_reddit_elite_influencers.csv" | cut -d',' -f4 | grep -c "^$tier$" || echo 0)
            if [ $count -gt 0 ]; then
              percentage=$(( count * 100 / max_accounts ))
              echo "  $tier: $count accounts (${percentage}%)"
            fi
          done
          
        else
          echo "âŒ No accounts found in any file - scraping may have failed completely"
        fi
        
        # Log file analysis
        if [ -f "elite_scraper.log" ]; then
          log_size=$(ls -lh elite_scraper.log | awk '{print $5}')
          echo "ğŸ“‹ Log file: $log_size"
          
          # Extract key metrics from log
          total_requests=$(grep -c "request\|GET" elite_scraper.log 2>/dev/null || echo "0")
          errors=$(grep -c "ERROR\|error\|Error" elite_scraper.log 2>/dev/null || echo "0")
          rate_limits=$(grep -c "rate limit\|Rate limit\|429" elite_scraper.log 2>/dev/null || echo "0")
          key_rotations=$(grep -c "rotating\|next key\|Key rotation" elite_scraper.log 2>/dev/null || echo "0")
          
          echo "ğŸ“ˆ Session Statistics:"
          echo "  â€¢ API requests: $total_requests"
          echo "  â€¢ Errors: $errors"
          echo "  â€¢ Rate limits: $rate_limits"  
          echo "  â€¢ Key rotations: $key_rotations"
        fi
        
        # Progress file analysis
        if [ -f "elite_scraper_progress.json" ]; then
          echo "ğŸ’¾ Progress file available for resumption"
          python3 -c "
import json
try:
    with open('elite_scraper_progress.json', 'r') as f:
        data = json.load(f)
    print(f'ğŸ“Š Session Progress:')
    print(f'  â€¢ Total processed: {len(data.get(\"scraped_users\", []))} users')
    print(f'  â€¢ Failed users: {len(data.get(\"failed_users\", []))} users')
    if 'multi_key_stats' in data:
        stats = data['multi_key_stats']
        print(f'  â€¢ Credits used: {stats.get(\"total_credits_used\", 0)}')
        print(f'  â€¢ Active keys: {stats.get(\"active_keys\", 0)}/{stats.get(\"total_keys\", 0)}')
except Exception as e:
    print(f'Could not analyze progress: {e}')
"
        fi
        
        echo "âœ¨ Analysis complete!"
    
    - name: ğŸ“¤ Upload ALL Results as Artifacts (Always)
      if: always()  # This ensures artifacts are uploaded regardless of success/failure
      uses: actions/upload-artifact@v4
      with:
        name: reddit-scraper-results-${{ github.run_number }}-${{ github.event.inputs.target_count || '2500' }}-accounts
        path: |
          reddit_elite_influencers.csv
          FINAL_reddit_elite_influencers.csv
          backup_reddit_elite_influencers.csv
          interrupted_reddit_elite_influencers.csv
          error_reddit_elite_influencers.csv
          elite_scraper_progress.json
          elite_scraper.log
          logs/scraper_output.log
          logs/
        retention-days: 30
        if-no-files-found: warn
    
    - name: ğŸ“¤ Upload Backup Results (Emergency)
      if: failure()  # Additional upload if main workflow fails
      uses: actions/upload-artifact@v4
      with:
        name: emergency-backup-${{ github.run_number }}
        path: |
          *.csv
          *.json
          *.log
          logs/
        retention-days: 60
        if-no-files-found: ignore
    
    - name: ğŸ’¾ Commit Results to Repository (Optional)
      if: success() && github.ref == 'refs/heads/main' && env.SCRAPER_STATUS == 'success'
      run: |
        # Configure git
        git config --local user.email "action@github.com"
        git config --local user.name "Reddit Scraper Bot"
        
        # Prepare commit message
        account_count=0
        if [ -f "reddit_elite_influencers.csv" ]; then
          account_count=$(($(wc -l < reddit_elite_influencers.csv) - 1))
        fi
        
        commit_msg="ğŸ¤– Auto-scrape: $account_count elite Reddit accounts ($(date +%Y-%m-%d))"
        
        # Add files
        git add reddit_elite_influencers.csv elite_scraper_progress.json
        
        # Commit with detailed message
        git commit -m "$commit_msg" -m "Runtime: $MAX_RUNTIME_HOURS hours | Target: $TARGET_COUNT | Status: $SCRAPER_STATUS" || {
          echo "No changes to commit"
          exit 0
        }
        
        # Push to repository
        git push || {
          echo "Push failed - this is normal if repository is protected"
          exit 0
        }
    
    - name: ğŸ“Š Final Performance Report
      if: always()
      run: |
        echo "â±ï¸ FINAL PERFORMANCE REPORT"
        echo "==========================="
        echo "ğŸ Workflow Status: ${SCRAPER_STATUS:-unknown}"
        echo "â° Requested Runtime: $MAX_RUNTIME_HOURS hours"
        echo "ğŸ¯ Target Count: $TARGET_COUNT accounts"
        echo "ğŸ’¾ Save Interval: $SAVE_INTERVAL accounts"
        
        # Count final results
        total_found=0
        if [ -f "reddit_elite_influencers.csv" ]; then
          total_found=$(($(wc -l < reddit_elite_influencers.csv) - 1))
        fi
        
        echo "ğŸ“Š Results Summary:"
        echo "  â€¢ Accounts scraped: $total_found"
        echo "  â€¢ Success rate: $(( total_found * 100 / TARGET_COUNT ))%"
        echo "  â€¢ Files created: $(ls -1 *.csv *.json *.log 2>/dev/null | wc -l)"
        
        if [ $total_found -gt 0 ]; then
          efficiency=$(( TARGET_COUNT * 100 / total_found ))
          echo "  â€¢ Target efficiency: ${efficiency}%"
          echo "âœ… SUCCESS: Data collected and uploaded as artifacts"
        else
          echo "âŒ WARNING: No data collected - check logs for issues"
        fi
        
        echo "ğŸ“ Artifacts uploaded:"
        echo "  â€¢ Main results: reddit-scraper-results-${{ github.run_number }}-${{ github.event.inputs.target_count || '2500' }}-accounts"
        if [ "${SCRAPER_STATUS:-unknown}" != "success" ]; then
          echo "  â€¢ Emergency backup: emergency-backup-${{ github.run_number }}"
        fi
        
        echo "ğŸ Workflow completed at $(date)"
        echo "âœ¨ Thank you for using Elite Reddit Scraper!"
